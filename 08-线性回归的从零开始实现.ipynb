{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "operator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01md2l\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m torch \u001b[38;5;28;01mas\u001b[39;00m d2l\n",
      "File \u001b[1;32md:\\software\\miniforge\\envs\\graphrag\\lib\\site-packages\\d2l\\torch.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "File \u001b[1;32md:\\software\\miniforge\\envs\\graphrag\\lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32md:\\software\\miniforge\\envs\\graphrag\\lib\\site-packages\\torchvision\\_meta_registrations.py:164\u001b[0m\n\u001b[0;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[0;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    158\u001b[0m         ),\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[0;32m    163\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision::nms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should have 4 elements in dimension 1, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\software\\miniforge\\envs\\graphrag\\lib\\site-packages\\torch\\library.py:795\u001b[0m, in \u001b[0;36mregister\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mregister_fake\u001b[39m(\n\u001b[0;32m    718\u001b[0m     op: _op_identifier,\n\u001b[0;32m    719\u001b[0m     func: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    723\u001b[0m     _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    724\u001b[0m ):\n\u001b[0;32m    725\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Register a FakeTensor implementation (\"fake impl\") for this operator.\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    Also sometimes known as a \"meta kernel\", \"abstract impl\".\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \n\u001b[0;32m    729\u001b[0m \u001b[38;5;124;03m    An \"FakeTensor implementation\" specifies the behavior of this operator on\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;124;03m    Tensors that carry no data (\"FakeTensor\"). Given some input Tensors with\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;124;03m    certain properties (sizes/strides/storage_offset/device), it specifies\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;124;03m    what the properties of the output Tensors are.\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \n\u001b[0;32m    734\u001b[0m \u001b[38;5;124;03m    The FakeTensor implementation has the same signature as the operator.\u001b[39;00m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m    It is run for both FakeTensors and meta tensors. To write a FakeTensor\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;124;03m    implementation, assume that all Tensor inputs to the operator are\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m    regular CPU/CUDA/Meta tensors, but they do not have storage, and\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;124;03m    you are trying to return regular CPU/CUDA/Meta tensor(s) as output.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;124;03m    The FakeTensor implementation must consist of only PyTorch operations\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03m    (and may not directly access the storage or data of any input or\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;124;03m    intermediate Tensors).\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \n\u001b[0;32m    743\u001b[0m \u001b[38;5;124;03m    This API may be used as a decorator (see examples).\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m    For a detailed guide on custom ops, please see\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m    https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m        >>> import torch\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;124;03m        >>> import numpy as np\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m        >>> from torch import Tensor\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m        >>> # Example 1: an operator without data-dependent output shape\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m        >>> @torch.library.custom_op(\"mylib::custom_linear\", mutates_args=())\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03m        >>> def custom_linear(x: Tensor, weight: Tensor, bias: Tensor) -> Tensor:\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m        >>>     raise NotImplementedError(\"Implementation goes here\")\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03m        >>> @torch.library.register_fake(\"mylib::custom_linear\")\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m        >>> def _(x, weight, bias):\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;124;03m        >>>     assert x.dim() == 2\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;124;03m        >>>     assert weight.dim() == 2\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;124;03m        >>>     assert bias.dim() == 1\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03m        >>>     assert x.shape[1] == weight.shape[1]\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m        >>>     assert weight.shape[0] == bias.shape[0]\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m        >>>     assert x.device == weight.device\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03m        >>>     return (x @ weight.t()) + bias\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;124;03m        >>> with torch._subclasses.fake_tensor.FakeTensorMode():\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;124;03m        >>>     x = torch.randn(2, 3)\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;124;03m        >>>     w = torch.randn(3, 3)\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;124;03m        >>>     b = torch.randn(3)\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;124;03m        >>>     y = torch.ops.mylib.custom_linear(x, w, b)\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;124;03m        >>> assert y.shape == (2, 3)\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m        >>> # Example 2: an operator with data-dependent output shape\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;124;03m        >>> @torch.library.custom_op(\"mylib::custom_nonzero\", mutates_args=())\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;124;03m        >>> def custom_nonzero(x: Tensor) -> Tensor:\u001b[39;00m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;124;03m        >>>     x_np = x.numpy(force=True)\u001b[39;00m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;124;03m        >>>     res = np.stack(np.nonzero(x_np), axis=1)\u001b[39;00m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;124;03m        >>>     return torch.tensor(res, device=x.device)\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;124;03m        >>> @torch.library.register_fake(\"mylib::custom_nonzero\")\u001b[39;00m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;124;03m        >>> def _(x):\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;124;03m        >>> # Number of nonzero-elements is data-dependent.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;124;03m        >>> # Since we cannot peek at the data in an fake impl,\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;124;03m        >>> # we use the ctx object to construct a new symint that\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;124;03m        >>> # represents the data-dependent size.\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;124;03m        >>>     ctx = torch.library.get_ctx()\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;124;03m        >>>     nnz = ctx.new_dynamic_size()\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;124;03m        >>>     shape = [nnz, x.dim()]\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;124;03m        >>>     result = x.new_empty(shape, dtype=torch.int64)\u001b[39;00m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;124;03m        >>>     return result\u001b[39;00m\n\u001b[1;32m--> 795\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;124;03m        >>> from torch.fx.experimental.proxy_tensor import make_fx\u001b[39;00m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;124;03m        >>> x = torch.tensor([0, 1, 2, 3, 4, 0])\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;124;03m        >>> trace = make_fx(torch.ops.mylib.custom_nonzero, tracing_mode=\"symbolic\")(x)\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;124;03m        >>> trace.print_readable()\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;124;03m        >>> assert torch.allclose(trace(x), torch.ops.mylib.custom_nonzero(x))\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    806\u001b[0m         op, (\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_ops\u001b[38;5;241m.\u001b[39mOpOverload, torch\u001b[38;5;241m.\u001b[39m_library\u001b[38;5;241m.\u001b[39mcustom_ops\u001b[38;5;241m.\u001b[39mCustomOpDef)\n\u001b[0;32m    807\u001b[0m     ):\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(op): got unexpected type for op: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mtype(op)}\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\software\\miniforge\\envs\\graphrag\\lib\\site-packages\\torch\\library.py:184\u001b[0m, in \u001b[0;36m_register_fake\u001b[1;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[0;32m    180\u001b[0m # Can be none if you call register_fake from somewhere there isn't a module\n\u001b[0;32m    181\u001b[0m # (e.g. __main__)\n\u001b[0;32m    182\u001b[0m caller_module_name = None if caller_module is None else caller_module.__name__\n\u001b[1;32m--> 184\u001b[0m # TODO(rzou): We're gonna need to stage this change with torchvision,\n\u001b[0;32m    185\u001b[0m # since torchvision is github first.\n\u001b[0;32m    186\u001b[0m if caller_module_name is not None and caller_module_name.startswith(\n\u001b[0;32m    187\u001b[0m     \"torchvision.\"\n\u001b[0;32m    188\u001b[0m ):\n\u001b[0;32m    189\u001b[0m     caller_module_name = None\n",
      "File \u001b[1;32md:\\software\\miniforge\\envs\\graphrag\\lib\\site-packages\\torch\\_library\\fake_impl.py:31\u001b[0m, in \u001b[0;36mFakeImplHolder.register\u001b[1;34m(self, func, source)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an fake impl registered at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m     )\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_has_kernel_for_dispatch_key(\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeImplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m ):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
